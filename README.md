# MLOps Demo Project (Bilingual: Portugu√™s/English)

---

## üìë Table of Contents

- [Portugu√™s](#portugu√™s)
- [English](#english)

---

<a name="portugu√™s"></a>
# üáßüá∑ Portugu√™s

## Objetivo do Projeto

Este projeto visa facilitar a vida de usu√°rios que desejam obter informa√ß√µes por chat com base em documentos PDFs que possuam.

```
fastapi, chromadb, langchain, streamlit, httpx, slate3k, docker
```

O objetivo √© criar um pipeline simples de RAG (Retrieval Augmented Generation) com LLM, demonstrando conceitos como banco vetorial, API, frontend interativo e automa√ß√£o.

---

## Estrutura do Projeto

```
lang_mlops/
‚îú‚îÄ‚îÄ backend/        # FastAPI, l√≥gica do LLM, ChromaDB, Langchain
‚îú‚îÄ‚îÄ frontend/       # Streamlit app para intera√ß√£o do usu√°rio
‚îú‚îÄ‚îÄ common/         # Utilidades e c√≥digo compartilhado
‚îú‚îÄ‚îÄ schemas/        # Schemas Pydantic para API
‚îú‚îÄ‚îÄ scripts/        # Scripts auxiliares
‚îú‚îÄ‚îÄ pdfreader.py    # Leitura de PDFs
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ README.md
```

- **backend/**: Endpoints FastAPI, integra√ß√£o com LLM (via Langchain) e banco vetorial (ChromaDB).
- **frontend/**: Aplica√ß√£o Streamlit para upload de PDFs, busca e intera√ß√£o com a LLM.

---

## Como Rodar o Projeto

### Pr√©-requisitos
- Python >= 3.10
- Docker & Docker Compose
- Arquivo `.env` na raiz do projeto com as vari√°veis necess√°rias

### Rodando o Projeto

Com o `.env` pronto, execute:

```sh
docker compose up -d
```

### Como Usar

- **Frontend (Streamlit):**
    - [http://localhost:8501](http://localhost:8501)
    - Fa√ßa upload de PDFs para adicionar contexto e fa√ßa perguntas baseadas neles.

- **Backend (FastAPI):**
    - Principais endpoints:
        - `/context/post` (POST): Adiciona contexto ao banco vetorial.
        - `/context/get` (GET): Busca contexto similar.
        - `/context/get/all` (GET): Retorna todo o contexto armazenado.
        - `/background` (POST): Define contexto de fundo para a LLM.
        - `/llm/response` (GET): Retorna resposta da LLM baseada no contexto.

---

## Funcionalidades

- [x] Upload de PDFs e extra√ß√£o de contexto (Streamlit)
- [x] Banco vetorial com ChromaDB
- [x] API com FastAPI para armazenamento e busca de contexto
- [x] Integra√ß√£o com LLM via Langchain
- [x] Dockeriza√ß√£o completa

---

## Fundamenta√ß√£o Te√≥rica

- **Vector DB (ChromaDB):** Utiliza embeddings para armazenar e buscar contexto textual.
- **RAG (Retrieval Augmented Generation):** T√©cnica para guiar respostas de LLMs com base em contexto externo.
- **Langchain:** Abstra√ß√£o para integra√ß√£o com LLMs e fluxos de RAG.

---

## O que foi aprendido

O projeto consolidou conhecimentos em orienta√ß√£o a objetos, RAG, embeddings, APIs, automa√ß√£o, integra√ß√£o de servi√ßos e dockeriza√ß√£o.


---

<a name="english"></a>
# üá¨üáß English

## Project Purpose

This project is intended for users who want to get information from their own PDF documents through a chat-like experience.

```
fastapi, chromadb, langchain, streamlit, httpx, slate3k, docker
```

The goal is to create a simple RAG pipeline with LLM, demonstrating concepts such as vector database, API, interactive frontend, and automation.

---

## Project Structure

```
lang_mlops/
‚îú‚îÄ‚îÄ backend/        # FastAPI, LLM logic, ChromaDB, Langchain
‚îú‚îÄ‚îÄ frontend/       # Streamlit app for user interaction
‚îú‚îÄ‚îÄ common/         # Utilities and shared code
‚îú‚îÄ‚îÄ schemas/        # Pydantic schemas for API
‚îú‚îÄ‚îÄ scripts/        # Auxiliary scripts
‚îú‚îÄ‚îÄ pdfreader.py    # PDF reading
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ README.md
```

- **backend/**: FastAPI endpoints, integration with LLM (via Langchain) and vector DB (ChromaDB).
- **frontend/**: Streamlit app for PDF upload, search, and LLM interaction.

---

## How to Run

### Prerequisites
- Python >= 3.10
- Docker & Docker Compose
- `.env` file at the project root with the required variables

### Running the Project

```

### Rodando o Projeto | Running the Project

Com o `.env` pronto, execute:
With the `.env` file ready, run:

```sh
docker compose up -d
```

### Como Usar | How to Use

- **Frontend (Streamlit):**
    - [http://localhost:8501](http://localhost:8501)
    - Fa√ßa upload de PDFs para adicionar contexto e fa√ßa perguntas baseadas neles.
    - Upload PDFs to add context and ask questions based on them.

- **Backend (FastAPI):**
    - Endpoints principais:
        - `/context/post` (POST): Adiciona contexto ao banco vetorial.
        - `/context/get` (GET): Busca contexto similar.
        - `/context/get/all` (GET): Retorna todo o contexto armazenado.
        - `/background` (POST): Define contexto de fundo para a LLM.
        - `/llm/response` (GET): Retorna resposta da LLM baseada no contexto.
    - Main endpoints:
        - `/context/post` (POST): Add context to the vector database.
        - `/context/get` (GET): Search for similar context.
        - `/context/get/all` (GET): Get all stored context.
        - `/background` (POST): Set background context for the LLM.
        - `/llm/response` (GET): Get LLM response based on context.

---

## Funcionalidades | Features

- [x] Upload de PDFs e extra√ß√£o de contexto (Streamlit)
- [x] Banco vetorial com ChromaDB
- [x] API com FastAPI para armazenamento e busca de contexto
- [x] Integra√ß√£o com LLM via Langchain
- [x] Dockeriza√ß√£o completa
- [ ] Orquestra√ß√£o autom√°tica com Airflow (em desenvolvimento)

---

## Fundamenta√ß√£o Te√≥rica | Theoretical Foundation

- **Vector DB (ChromaDB):**
    - Utiliza embeddings para armazenar e buscar contexto textual.
    - Uses embeddings to store and search textual context.

- **RAG (Retrieval Augmented Generation):**
    - T√©cnica para guiar respostas de LLMs com base em contexto externo.
    - Technique to guide LLM responses based on external context.

- **Langchain:**
    - Abstra√ß√£o para integra√ß√£o com LLMs e fluxos de RAG.
    - Abstraction for integrating with LLMs and RAG pipelines.

---

## O que foi aprendido | What I Learned

O projeto consolidou conhecimentos em orienta√ß√£o a objetos, RAG, embeddings, APIs, automa√ß√£o, integra√ß√£o de servi√ßos e dockeriza√ß√£o.
The project consolidated knowledge in OOP, RAG, embeddings, APIs, automation, service integration, and dockerization.



- Aqui voce vera os esqueletos de cada endpoint do fastapi que o modelo esta utilizando para a llm 
[Endpoints da llm fastapi](http://localhost:8000)